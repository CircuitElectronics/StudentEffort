{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XeRInB4gB71x"
   },
   "source": [
    "# Bias in Artificial Intelligence\n",
    "**Author: ilia Khosravi Khorashad**\n",
    "**contact address: ilia.khosravikhorashad@mail.um.ac.ir**\n",
    "\n",
    "## Introduction\n",
    "In the world of programming and algorithms, specific code is written to solve each problem. However, in artificial intelligence, problems are solved with one algorithm, and the main difference lies in the data used. Therefore in AI, one of the most important factors is having appropriate, sufficient, and classified data.\n",
    "\n",
    "One of the key issues that needs attention in AI to avoid errors is bias. Bias can occur for various reasons. In some cases, it can be beneficial, such as classifying data based on specific parameters. but, in most cases, bias is undesirable. For instance, in image processing, a person’s skin color should not affect the outcome of the model.\n",
    "\n",
    "\n",
    "![example](./images/example.png)\n",
    "\n",
    "\n",
    "in this example of gender bias, adapted from a report published by researchers deom University of Virginia and the University of Washington, a visual semantic role labeling system has learned to identify a person cooking as female, even when the image is male.\n",
    "\n",
    "## Why Should We Eliminate Bias in AI?\n",
    "Eliminating bias in machine learning systems is crucial because they learn from data. If the data contains bias, the results will also be skewed. This is particularly important in supervised learning, where high-quality and unbiased data is essential.\n",
    "\n",
    "AI systems with bias can cause issues, especially in automated decision-making, autonomous operations, and facial recognition software. For example, Google has misidentified images of minorities, gender discrimination has occurred in credit rating apps, and racial biases have appeared in criminal conviction software.\n",
    "\n",
    "These errors can harm individuals and businesses, reduce trust in AI technology, and lead to legal and financial problems.\n",
    "\n",
    "![bias](./images/bias.png)\n",
    "\n",
    "## How to Identify Bias in AI\n",
    "Identifying and measuring bias in AI is challenging especially with deep learning algorithms that operate like a \"black box.\" This means it’s difficult to understand which part of the data the model is working on, and only the results are visible. Due to this opacity, it is hard to detect biases. This is why researchers are developing Explainable AI (XAI), aiming to increase transparency in AI models and turn the \"black box\" into a \"white box.\"\n",
    "\n",
    "Example: Image Classification with a Neural Network (Black Box) to an Explainable Model (White Box)\n",
    "Black Box Scenario:\n",
    "Task: An AI model (e.g., a deep neural network) is trained to classify medical images, such as distinguishing between images of healthy lungs and those with pneumonia.\n",
    "Problem: The deep learning model achieves high accuracy but does not provide any explanation about why it classified an image a certain way. Doctors are hesitant to trust the model because they cannot see what features (parts of the image) the model focused on to make its decision.\n",
    "Turning the Black Box into a White Box:\n",
    "To make this model more explainable, researchers can apply various Explainable AI (XAI) techniques that provide transparency. One commonly used method is Grad-CAM (Gradient-weighted Class Activation Mapping). Here's how this would work:\n",
    "\n",
    "Model Explanation Using Grad-CAM:\n",
    "\n",
    "Grad-CAM visualizes the areas of the image that are most important for the model's decision.\n",
    "After the model classifies an image, Grad-CAM creates a heatmap overlay on the image, highlighting the regions that the neural network paid the most attention to when making the decision. For example, if the model classified an image as showing pneumonia, the heatmap might highlight areas in the lungs where abnormal features (such as fluid buildup) are present.\n",
    "Results of Explainability:\n",
    "\n",
    "The doctor can now see the exact areas of the X-ray where the AI model detected patterns associated with pneumonia.\n",
    "If the heatmap corresponds to regions where human doctors would also look for pneumonia, it increases trust in the model’s decisions.\n",
    "Outcome:\n",
    "\n",
    "The model is no longer a black box because users (like doctors) can understand why the AI made its decision. They have a visual explanation of what the model focused on, making the AI more transparent and interpretable. Thus, the black box is effectively turned into a white box.\n",
    "\n",
    "![pneumonia Grad-CAM](./images/pneumonia_Grad.png)\n",
    "\n",
    "Bias in data often arises due to the inappropriate selection of training datasets. If the dataset is limited to a subset of the population, the model will provide incorrect results in real-world scenarios. Companies are working to increase diversity in datasets to combat these biases and improve machine learning models.\n",
    "\n",
    "## Types of Biases\n",
    "1. **Sampling Bias**: Occurs when one feature is overrepresented in data collection compared to others. Sampling should either be random or representative of the population to avoid bias.\n",
    "\n",
    "\n",
    "![measurement bias](./images/sampling_bias.png)\n",
    "\n",
    "2. **Measurement Bias**: Occurs when data is not measured or recorded correctly. For example, regional differences in employee salaries could affect data measurement in salary estimation models.\n",
    "\n",
    "![measurement bias](./images/measurement_bias.jpg)\n",
    "\n",
    "3. **Exclusion Bias**: Similar to sampling bias, exclusion bias arises from data that's inappropriately removed from the data source. When you have petabytes or more of data, it's tempting to select a small sample to use for training -- but in doing so, you might inadvertently exclude certain data, resulting in a biased data set. Exclusion bias also happens when duplicates are removed from data where the data elements are actually distinct.\n",
    "\n",
    "4. **Prejudicial Bias**: Refers to human prejudice. When using historical data to train models, care must be taken to ensure previous biases do not transfer to new models.\n",
    "5. **Bandwagon Effect**: Happens when a trend in data grows, leading to more data collection around that trend. This can overemphasize an idea and introduce bias into models.\n",
    "\n",
    "## Solutions to Eliminate Bias (Increasing Fairness in Models)\n",
    "1. **Feature Blinding**:  Feature blinding involves removing attributes as inputs in models -- in other words, blinding the model to specific features or protected attributes such as race and gender. However, this method isn't always sufficient, as other attributes can remain that correlate with different genders or races, enabling the model to develop a bias. For instance, certain genders might correlate with specific types of cars.\n",
    "2. **Objective Function Modification**: Instead of optimizing for accuracy alone, this method adjusts the model’s objective function to improve fairness.\n",
    "3. **Adversarial Classification**: Adversarial classification involves optimizing a model not just for accurate predictions, but also inaccurate predictions. Though it might sound counterintuitive, poor predictions point out weak spots in a model, and then the model can be optimized to prevent those weaknesses.\n",
    "\n",
    "These techniques help reduce bias and improve fairness in machine learning models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example in production\n",
    "\n",
    "Here is an example of AI bias: the problem arises because we have more normal images than pneumonia images in the dataset. Due to the imbalance in the number of images, the model becomes biased in its results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install tensorflow-addons\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "# from spp.SpatialPyramidPooling import SpatialPyramidPooling # Ilia\n",
    "\n",
    "from tensorflow.keras.metrics import FalsePositives, FalseNegatives, TrueNegatives, TruePositives\n",
    "\n",
    "batch_size = 32\n",
    "img_height = 100\n",
    "img_width = 143\n",
    "\n",
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "  'Data',\n",
    "  validation_split=0.1,\n",
    "  subset=\"training\",\n",
    "  seed=123,\n",
    "  # image_size=(img_height, img_width),\n",
    "  batch_size=batch_size\n",
    ")\n",
    "\n",
    "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "  'Data',\n",
    "  validation_split=0.1,\n",
    "  subset=\"validation\",\n",
    "  seed=123,\n",
    "  # image_size=(img_height, img_width),\n",
    "  batch_size=batch_size\n",
    ")\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "  layers.experimental.preprocessing.Rescaling(1./255),\n",
    "  layers.Conv2D(2, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Conv2D(4, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Conv2D(8, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Conv2D(16, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  layers.Conv2D(32, 3, padding='same', activation='relu'),\n",
    "  layers.MaxPooling2D(),\n",
    "  tfa.layers.SpatialPyramidPooling2D([1, 2, 4]),\n",
    "  layers.Flatten(),\n",
    "  #layers.SpatialPyramidPooling([1, 2, 4]), # Ilia\n",
    "\n",
    "  layers.Dense(32, activation='relu'),\n",
    "  layers.Dense(1, activation='sigmoid')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "\n",
    "#EPOCHS = 6\n",
    "#earlyStopping = EarlyStopping(monitor='val_loss', patience=10, verbose=0, mode='min')\n",
    "#mcp_save = ModelCheckpoint('.mdl_wts.hdf5', save_best_only=True, monitor='val_loss', mode='min')\n",
    "#reduce_lr_loss = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=7, verbose=1, epsilon=1e-4, mode='min')\n",
    "\n",
    "#model.compile(optimizer='rmsprop',\n",
    "#             loss='binary_crossentropy',\n",
    "#              metrics=['accuracy'])\n",
    "\n",
    "\n",
    "#history = model.fit(\n",
    "#        train_ds,\n",
    "#        validation_data=val_ds,\n",
    "#        epochs=EPOCHS,\n",
    "#        verbose=1,\n",
    "#        callbacks=[earlyStopping, mcp_save, reduce_lr_loss],\n",
    "#    )\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy', FalsePositives(), FalseNegatives(), TrueNegatives(), TruePositives()])\n",
    "\n",
    "history = model.fit(\n",
    "  train_ds,\n",
    "  validation_data=val_ds,\n",
    "  epochs=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "predictions = np.concatenate(model.predict(val_ds).round())\n",
    "labels = np.concatenate([y for x, y in val_ds])\n",
    "test_acc = sum(predictions == labels) / len(labels)\n",
    "print(test_acc)\n",
    "print(val_ds.class_names)\n",
    "\n",
    "model.evaluate(val_ds)\n",
    "\n",
    "tf.math.confusion_matrix(labels, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "When the dataset consists of 35% abnormal images and 65% normal images, the model achieves an accuracy of 76%. However, when the distribution is 55% normal images and 45% abnormal images, the accuracy increases to 93%.\n",
    "\n",
    "![chest_XRay](./images/chest_x_ray.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source:\n",
    "https://news.mit.edu/2022/fairness-accuracy-ai-models-0720\n",
    "\n",
    "https://www.techtarget.com/searchenterpriseai/feature/6-ways-to-reduce-different-types-of-bias-in-machine-learning\n",
    "\n",
    "https://syntheticus.ai/blog/tackling-bias-in-large-ml-models-the-role-of-synthetic-data\n",
    "\n",
    "Mitchell, T. M. (1980), The need for biases in learning generalizations, CBM-TR 5-110, New Brunswick, New Jersey, USA: Rutgers University, CiteSeerX 10.1.1.19.5466"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
